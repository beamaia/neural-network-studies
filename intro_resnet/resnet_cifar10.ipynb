{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will follow a [PyTorch tutorial](https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/) implementing [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) with CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing modules (see more about this later)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='../cifar-10-batches-py/',\n",
    "                                             train=True, \n",
    "                                             transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../cifar-10-batches-py/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ../cifar-10-batches-py/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Pad(padding=4, fill=0, padding_mode=constant)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomCrop(size=(32, 32), padding=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../cifar-10-batches-py/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fba0483cf10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_dataset.data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True) #Original tutorial has shuffle=True\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f43805b3e20>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels,\n",
    "                     out_channels,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(1, 16)  # 1 when using mnist, 3 when using cifar10\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        \n",
    "        downsample = None\n",
    "        \n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-42-b1681609b042>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-b1681609b042>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for i, (images, labels) in enumerate(train_loader):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def train_model(train_loader, num_epoch, model, error, optimizer, curr_lr, update_lr):\n",
    "    print(\"Training model\")\n",
    "    print(f\"Total epochs: {num_epoch}. Current learning rate: {curr_lr}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.resize_(100, 1, 32, 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.resize_(100, 1, 32, 32).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters (attempt changing them later)\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_args = {\n",
    "    \"block\" : ResidualBlock,\n",
    "    \"layers\": [2, 2, 2]\n",
    "}\n",
    "\n",
    "model = ResNet(**net_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 2.3057\n",
      "Epoch [1/80], Step [200/500] Loss: 2.3005\n",
      "Epoch [1/80], Step [300/500] Loss: 2.3031\n",
      "Epoch [1/80], Step [400/500] Loss: 2.3033\n",
      "Epoch [1/80], Step [500/500] Loss: 2.3057\n",
      "Epoch [2/80], Step [100/500] Loss: 2.2999\n",
      "Epoch [2/80], Step [200/500] Loss: 2.3021\n",
      "Epoch [2/80], Step [300/500] Loss: 2.3047\n",
      "Epoch [2/80], Step [400/500] Loss: 2.3029\n",
      "Epoch [2/80], Step [500/500] Loss: 2.3016\n",
      "Epoch [3/80], Step [100/500] Loss: 2.3040\n",
      "Epoch [3/80], Step [200/500] Loss: 2.3014\n",
      "Epoch [3/80], Step [300/500] Loss: 2.3014\n",
      "Epoch [3/80], Step [400/500] Loss: 2.3028\n",
      "Epoch [3/80], Step [500/500] Loss: 2.3034\n",
      "Epoch [4/80], Step [100/500] Loss: 2.3042\n",
      "Epoch [4/80], Step [200/500] Loss: 2.3072\n",
      "Epoch [4/80], Step [300/500] Loss: 2.3004\n",
      "Epoch [4/80], Step [400/500] Loss: 2.3043\n",
      "Epoch [4/80], Step [500/500] Loss: 2.3006\n",
      "Epoch [5/80], Step [100/500] Loss: 2.2984\n",
      "Epoch [5/80], Step [200/500] Loss: 2.3048\n",
      "Epoch [5/80], Step [300/500] Loss: 2.3076\n",
      "Epoch [5/80], Step [400/500] Loss: 2.3059\n",
      "Epoch [5/80], Step [500/500] Loss: 2.3009\n",
      "Epoch [6/80], Step [100/500] Loss: 2.3048\n",
      "Epoch [6/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [6/80], Step [300/500] Loss: 2.3058\n",
      "Epoch [6/80], Step [400/500] Loss: 2.3050\n",
      "Epoch [6/80], Step [500/500] Loss: 2.3064\n",
      "Epoch [7/80], Step [100/500] Loss: 2.3044\n",
      "Epoch [7/80], Step [200/500] Loss: 2.3040\n",
      "Epoch [7/80], Step [300/500] Loss: 2.3034\n",
      "Epoch [7/80], Step [400/500] Loss: 2.3042\n",
      "Epoch [7/80], Step [500/500] Loss: 2.3032\n",
      "Epoch [8/80], Step [100/500] Loss: 2.3047\n",
      "Epoch [8/80], Step [200/500] Loss: 2.2990\n",
      "Epoch [8/80], Step [300/500] Loss: 2.3009\n",
      "Epoch [8/80], Step [400/500] Loss: 2.3045\n",
      "Epoch [8/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [9/80], Step [100/500] Loss: 2.3018\n",
      "Epoch [9/80], Step [200/500] Loss: 2.3017\n",
      "Epoch [9/80], Step [300/500] Loss: 2.3030\n",
      "Epoch [9/80], Step [400/500] Loss: 2.3016\n",
      "Epoch [9/80], Step [500/500] Loss: 2.3041\n",
      "Epoch [10/80], Step [100/500] Loss: 2.3030\n",
      "Epoch [10/80], Step [200/500] Loss: 2.3030\n",
      "Epoch [10/80], Step [300/500] Loss: 2.3009\n",
      "Epoch [10/80], Step [400/500] Loss: 2.3046\n",
      "Epoch [10/80], Step [500/500] Loss: 2.3025\n",
      "Epoch [11/80], Step [100/500] Loss: 2.3035\n",
      "Epoch [11/80], Step [200/500] Loss: 2.3016\n",
      "Epoch [11/80], Step [300/500] Loss: 2.3032\n",
      "Epoch [11/80], Step [400/500] Loss: 2.3021\n",
      "Epoch [11/80], Step [500/500] Loss: 2.3014\n",
      "Epoch [12/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [12/80], Step [200/500] Loss: 2.3033\n",
      "Epoch [12/80], Step [300/500] Loss: 2.3028\n",
      "Epoch [12/80], Step [400/500] Loss: 2.3044\n",
      "Epoch [12/80], Step [500/500] Loss: 2.3040\n",
      "Epoch [13/80], Step [100/500] Loss: 2.3018\n",
      "Epoch [13/80], Step [200/500] Loss: 2.3020\n",
      "Epoch [13/80], Step [300/500] Loss: 2.3002\n",
      "Epoch [13/80], Step [400/500] Loss: 2.3041\n",
      "Epoch [13/80], Step [500/500] Loss: 2.3029\n",
      "Epoch [14/80], Step [100/500] Loss: 2.3037\n",
      "Epoch [14/80], Step [200/500] Loss: 2.3066\n",
      "Epoch [14/80], Step [300/500] Loss: 2.3017\n",
      "Epoch [14/80], Step [400/500] Loss: 2.3035\n",
      "Epoch [14/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [15/80], Step [100/500] Loss: 2.3029\n",
      "Epoch [15/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [15/80], Step [300/500] Loss: 2.3020\n",
      "Epoch [15/80], Step [400/500] Loss: 2.3030\n",
      "Epoch [15/80], Step [500/500] Loss: 2.3043\n",
      "Epoch [16/80], Step [100/500] Loss: 2.3011\n",
      "Epoch [16/80], Step [200/500] Loss: 2.3028\n",
      "Epoch [16/80], Step [300/500] Loss: 2.3030\n",
      "Epoch [16/80], Step [400/500] Loss: 2.3023\n",
      "Epoch [16/80], Step [500/500] Loss: 2.3039\n",
      "Epoch [17/80], Step [100/500] Loss: 2.3015\n",
      "Epoch [17/80], Step [200/500] Loss: 2.3033\n",
      "Epoch [17/80], Step [300/500] Loss: 2.3037\n",
      "Epoch [17/80], Step [400/500] Loss: 2.3014\n",
      "Epoch [17/80], Step [500/500] Loss: 2.3033\n",
      "Epoch [18/80], Step [100/500] Loss: 2.3023\n",
      "Epoch [18/80], Step [200/500] Loss: 2.3014\n",
      "Epoch [18/80], Step [300/500] Loss: 2.3021\n",
      "Epoch [18/80], Step [400/500] Loss: 2.3019\n",
      "Epoch [18/80], Step [500/500] Loss: 2.3043\n",
      "Epoch [19/80], Step [100/500] Loss: 2.3037\n",
      "Epoch [19/80], Step [200/500] Loss: 2.3024\n",
      "Epoch [19/80], Step [300/500] Loss: 2.3015\n",
      "Epoch [19/80], Step [400/500] Loss: 2.3017\n",
      "Epoch [19/80], Step [500/500] Loss: 2.3028\n",
      "Epoch [20/80], Step [100/500] Loss: 2.3029\n",
      "Epoch [20/80], Step [200/500] Loss: 2.3006\n",
      "Epoch [20/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [20/80], Step [400/500] Loss: 2.3017\n",
      "Epoch [20/80], Step [500/500] Loss: 2.3025\n",
      "Epoch [21/80], Step [100/500] Loss: 2.3021\n",
      "Epoch [21/80], Step [200/500] Loss: 2.3033\n",
      "Epoch [21/80], Step [300/500] Loss: 2.3013\n",
      "Epoch [21/80], Step [400/500] Loss: 2.3028\n",
      "Epoch [21/80], Step [500/500] Loss: 2.3028\n",
      "Epoch [22/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [22/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [22/80], Step [300/500] Loss: 2.3030\n",
      "Epoch [22/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [22/80], Step [500/500] Loss: 2.3030\n",
      "Epoch [23/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [23/80], Step [200/500] Loss: 2.3028\n",
      "Epoch [23/80], Step [300/500] Loss: 2.3028\n",
      "Epoch [23/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [23/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [24/80], Step [100/500] Loss: 2.3033\n",
      "Epoch [24/80], Step [200/500] Loss: 2.3029\n",
      "Epoch [24/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [24/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [24/80], Step [500/500] Loss: 2.3025\n",
      "Epoch [25/80], Step [100/500] Loss: 2.3021\n",
      "Epoch [25/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [25/80], Step [300/500] Loss: 2.3029\n",
      "Epoch [25/80], Step [400/500] Loss: 2.3029\n",
      "Epoch [25/80], Step [500/500] Loss: 2.3029\n",
      "Epoch [26/80], Step [100/500] Loss: 2.3024\n",
      "Epoch [26/80], Step [200/500] Loss: 2.3022\n",
      "Epoch [26/80], Step [300/500] Loss: 2.3019\n",
      "Epoch [26/80], Step [400/500] Loss: 2.3024\n",
      "Epoch [26/80], Step [500/500] Loss: 2.3028\n",
      "Epoch [27/80], Step [100/500] Loss: 2.3032\n",
      "Epoch [27/80], Step [200/500] Loss: 2.3028\n",
      "Epoch [27/80], Step [300/500] Loss: 2.3029\n",
      "Epoch [27/80], Step [400/500] Loss: 2.3024\n",
      "Epoch [27/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [28/80], Step [100/500] Loss: 2.3021\n",
      "Epoch [28/80], Step [200/500] Loss: 2.3031\n",
      "Epoch [28/80], Step [300/500] Loss: 2.3032\n",
      "Epoch [28/80], Step [400/500] Loss: 2.3032\n",
      "Epoch [28/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [29/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [29/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [29/80], Step [300/500] Loss: 2.3031\n",
      "Epoch [29/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [29/80], Step [500/500] Loss: 2.3025\n",
      "Epoch [30/80], Step [100/500] Loss: 2.3024\n",
      "Epoch [30/80], Step [200/500] Loss: 2.3023\n",
      "Epoch [30/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [30/80], Step [400/500] Loss: 2.3024\n",
      "Epoch [30/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [31/80], Step [100/500] Loss: 2.3032\n",
      "Epoch [31/80], Step [200/500] Loss: 2.3030\n",
      "Epoch [31/80], Step [300/500] Loss: 2.3032\n",
      "Epoch [31/80], Step [400/500] Loss: 2.3022\n",
      "Epoch [31/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [32/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [32/80], Step [200/500] Loss: 2.3024\n",
      "Epoch [32/80], Step [300/500] Loss: 2.3024\n",
      "Epoch [32/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [32/80], Step [500/500] Loss: 2.3028\n",
      "Epoch [33/80], Step [100/500] Loss: 2.3024\n",
      "Epoch [33/80], Step [200/500] Loss: 2.3023\n",
      "Epoch [33/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [33/80], Step [400/500] Loss: 2.3033\n",
      "Epoch [33/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [34/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [34/80], Step [200/500] Loss: 2.3022\n",
      "Epoch [34/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [34/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [34/80], Step [500/500] Loss: 2.3024\n",
      "Epoch [35/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [35/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [35/80], Step [300/500] Loss: 2.3028\n",
      "Epoch [35/80], Step [400/500] Loss: 2.3021\n",
      "Epoch [35/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [36/80], Step [100/500] Loss: 2.3021\n",
      "Epoch [36/80], Step [200/500] Loss: 2.3024\n",
      "Epoch [36/80], Step [300/500] Loss: 2.3022\n",
      "Epoch [36/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [36/80], Step [500/500] Loss: 2.3025\n",
      "Epoch [37/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [37/80], Step [200/500] Loss: 2.3029\n",
      "Epoch [37/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [37/80], Step [400/500] Loss: 2.3024\n",
      "Epoch [37/80], Step [500/500] Loss: 2.3029\n",
      "Epoch [38/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [38/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [38/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [38/80], Step [400/500] Loss: 2.3028\n",
      "Epoch [38/80], Step [500/500] Loss: 2.3025\n",
      "Epoch [39/80], Step [100/500] Loss: 2.3024\n",
      "Epoch [39/80], Step [200/500] Loss: 2.3027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [39/80], Step [400/500] Loss: 2.3032\n",
      "Epoch [39/80], Step [500/500] Loss: 2.3024\n",
      "Epoch [40/80], Step [100/500] Loss: 2.3029\n",
      "Epoch [40/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [40/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [40/80], Step [400/500] Loss: 2.3030\n",
      "Epoch [40/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [41/80], Step [100/500] Loss: 2.3022\n",
      "Epoch [41/80], Step [200/500] Loss: 2.3021\n",
      "Epoch [41/80], Step [300/500] Loss: 2.3023\n",
      "Epoch [41/80], Step [400/500] Loss: 2.3029\n",
      "Epoch [41/80], Step [500/500] Loss: 2.3029\n",
      "Epoch [42/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [42/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [42/80], Step [300/500] Loss: 2.3023\n",
      "Epoch [42/80], Step [400/500] Loss: 2.3028\n",
      "Epoch [42/80], Step [500/500] Loss: 2.3030\n",
      "Epoch [43/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [43/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [43/80], Step [300/500] Loss: 2.3023\n",
      "Epoch [43/80], Step [400/500] Loss: 2.3031\n",
      "Epoch [43/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [44/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [44/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [44/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [44/80], Step [400/500] Loss: 2.3028\n",
      "Epoch [44/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [45/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [45/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [45/80], Step [300/500] Loss: 2.3029\n",
      "Epoch [45/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [45/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [46/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [46/80], Step [200/500] Loss: 2.3028\n",
      "Epoch [46/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [46/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [46/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [47/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [47/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [47/80], Step [300/500] Loss: 2.3024\n",
      "Epoch [47/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [47/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [48/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [48/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [48/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [48/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [48/80], Step [500/500] Loss: 2.3025\n",
      "Epoch [49/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [49/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [49/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [49/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [49/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [50/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [50/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [50/80], Step [300/500] Loss: 2.3023\n",
      "Epoch [50/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [50/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [51/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [51/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [51/80], Step [300/500] Loss: 2.3024\n",
      "Epoch [51/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [51/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [52/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [52/80], Step [200/500] Loss: 2.3024\n",
      "Epoch [52/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [52/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [52/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [53/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [53/80], Step [200/500] Loss: 2.3028\n",
      "Epoch [53/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [53/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [53/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [54/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [54/80], Step [200/500] Loss: 2.3024\n",
      "Epoch [54/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [54/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [54/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [55/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [55/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [55/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [55/80], Step [400/500] Loss: 2.3025\n",
      "Epoch [55/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [56/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [56/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [56/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [56/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [56/80], Step [500/500] Loss: 2.3027\n",
      "Epoch [57/80], Step [100/500] Loss: 2.3024\n",
      "Epoch [57/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [57/80], Step [300/500] Loss: 2.3030\n",
      "Epoch [57/80], Step [400/500] Loss: 2.3028\n",
      "Epoch [57/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [58/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [58/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [58/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [58/80], Step [400/500] Loss: 2.3028\n",
      "Epoch [58/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [59/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [59/80], Step [200/500] Loss: 2.3023\n",
      "Epoch [59/80], Step [300/500] Loss: 2.3029\n",
      "Epoch [59/80], Step [400/500] Loss: 2.3024\n",
      "Epoch [59/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [60/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [60/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [60/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [60/80], Step [400/500] Loss: 2.3025\n",
      "Epoch [60/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [61/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [61/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [61/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [61/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [61/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [62/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [62/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [62/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [62/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [62/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [63/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [63/80], Step [200/500] Loss: 2.3027\n",
      "Epoch [63/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [63/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [63/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [64/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [64/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [64/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [64/80], Step [400/500] Loss: 2.3025\n",
      "Epoch [64/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [65/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [65/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [65/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [65/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [65/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [66/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [66/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [66/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [66/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [66/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [67/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [67/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [67/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [67/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [67/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [68/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [68/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [68/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [68/80], Step [400/500] Loss: 2.3027\n",
      "Epoch [68/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [69/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [69/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [69/80], Step [300/500] Loss: 2.3025\n",
      "Epoch [69/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [69/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [70/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [70/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [70/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [70/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [70/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [71/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [71/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [71/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [71/80], Step [400/500] Loss: 2.3025\n",
      "Epoch [71/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [72/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [72/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [72/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [72/80], Step [400/500] Loss: 2.3025\n",
      "Epoch [72/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [73/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [73/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [73/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [73/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [73/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [74/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [74/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [74/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [74/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [74/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [75/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [75/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [75/80], Step [300/500] Loss: 2.3027\n",
      "Epoch [75/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [75/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [76/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [76/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [76/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [76/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [76/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [77/80], Step [100/500] Loss: 2.3027\n",
      "Epoch [77/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [77/80], Step [300/500] Loss: 2.3026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [77/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [78/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [78/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [78/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [78/80], Step [400/500] Loss: 2.3026\n",
      "Epoch [78/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [79/80], Step [100/500] Loss: 2.3025\n",
      "Epoch [79/80], Step [200/500] Loss: 2.3025\n",
      "Epoch [79/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [79/80], Step [400/500] Loss: 2.3025\n",
      "Epoch [79/80], Step [500/500] Loss: 2.3026\n",
      "Epoch [80/80], Step [100/500] Loss: 2.3026\n",
      "Epoch [80/80], Step [200/500] Loss: 2.3026\n",
      "Epoch [80/80], Step [300/500] Loss: 2.3026\n",
      "Epoch [80/80], Step [400/500] Loss: 2.3025\n",
      "Epoch [80/80], Step [500/500] Loss: 2.3026\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.resize_(100, 1, 32, 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = error(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 10.24 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.resize_(100, 1, 32, 32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 10.0 %\n"
     ]
    }
   ],
   "source": [
    "test_model(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet_cifar10_v1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"color: blue\"><b>Study session observations: </b></span>\n",
    "\n",
    "<!-- #### 16/07/2020:\n",
    "Attempting to use the same resnet model to train CIFAR10. -->\n",
    "\n",
    "#### 17/07/2020-20/07/2020:\n",
    "Both models (with 3 layers) had a accuracy of ~10%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
