{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll be tweaking [kaggle notebook ResNet for MNIST with PyTorch](https://www.kaggle.com/readilen/resnet-for-mnist-with-pytorch?scriptVersionId=6942243) and the [PyTorch Tutorial on implementation of a ResNet model](https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/) in order to learn more about the ResNet model and also how to use PyTorch. The purpose is to change, add, remove, certain parts of the code and see exactly what happens, while also trying to better the accuracy (it's aprox ~10% in the other notebook that uses this model and the CIFAR 10 dataset). \n",
    "\n",
    "Code is due to [Liu Kuangs's extensive code](https://github.com/kuangliu) and also [TorchVision ResNet model](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"color: blue\"><b>Study session observations: </b></span>\n",
    "\n",
    "#### 20/07/2020:\n",
    "Had difficulties implementing a 4th layer due to size, during forward propagation of residual block. Will look more into that after a more concise understanding of the archictecture and classes used. Changed the notebook's name in order to be more incisive with the notebook content. \n",
    "\n",
    "Will attempt to change a few input values of certain variables (like learning_rate, train and test loader batch size, etc.) and also both forward methods to see what happens. \n",
    "\n",
    "Also want to see more about num_epochs needed and in general more about epochs. When using 80 epochs, the accuracy was of ~10-11%. With 1, ~9-10%. Want to see what can influence the accuracy of the model.\n",
    "\n",
    "#### 21/07/2020:\n",
    "Attempted to train model using different layers ([3, 4, 6, 3] instead of [2,2,2,2]). The forth value of the layers isn't used (yet). Tried also changing the value of num of epochs and the learning_rate. Tried understanding more about the resnet class from torchvision and each layer by reading a bit more of its source.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='../cifar-10-batches-py/',\n",
    "                                             train=True, \n",
    "                                             transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../cifar-10-batches-py/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.data.shape)\n",
    "print(test_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True) #Original tutorial has shuffle=True\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating classes and functions related to ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels,\n",
    "                     out_channels,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(1, 16)  # 1 when using mnist, 3 when using cifar10\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        print(\"\\nlayer1\")\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        print(\"\\nlayer2\")\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        print(\"\\nlayer3\")\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "#         print(\"\\nlayer4\")\n",
    "#         self.layer4 = self.make_layer(block, 128, layers[3], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        \n",
    "        downsample = None\n",
    "        \n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)  # unsure what this is, possibily flatten?\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetX(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNetX, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(1, 16)  # 1 when using mnist, 3 when using cifar10\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        print(\"\\nlayer1\")\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        print(\"\\nlayer2\")\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        print(\"\\nlayer3\")\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "#         print(\"\\nlayer4\")\n",
    "#         self.layer4 = self.make_layer(block, 128, layers[3], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        \n",
    "        downsample = None\n",
    "        \n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)  # unsure what this is, possibily flatten?\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, num_epochs, error, optimizer, curr_lr, total_step, device):\n",
    "    print(\"Training model\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.resize_(100, 1, 32, 32).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = error(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        # Decay learning rate\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            curr_lr /= 3\n",
    "            update_lr(optimizer, curr_lr)\n",
    "\n",
    "        print(\"Current learning rate: \", curr_lr)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layer1\n",
      "\n",
      "layer2\n",
      "\n",
      "layer3\n"
     ]
    }
   ],
   "source": [
    "net_args = {\n",
    "    \"block\" : ResidualBlock,\n",
    "    \"layers\": [2, 2, 2, 2]\n",
    "}\n",
    "\n",
    "model1 = ResNet(**net_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layer1\n",
      "\n",
      "layer2\n",
      "\n",
      "layer3\n"
     ]
    }
   ],
   "source": [
    "net_args = {\n",
    "    \"block\" : ResidualBlock,\n",
    "    \"layers\": [3, 4, 6, 3]\n",
    "}\n",
    "\n",
    "model4 = ResNet(**net_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Epoch [1/10], Step [100/500] Loss: 2.5775\n",
      "Epoch [1/10], Step [200/500] Loss: 2.4368\n",
      "Epoch [1/10], Step [300/500] Loss: 2.5746\n",
      "Epoch [1/10], Step [400/500] Loss: 2.5905\n",
      "Epoch [1/10], Step [500/500] Loss: 2.5330\n",
      "Current learning rate:  0.001\n",
      "Epoch [2/10], Step [100/500] Loss: 2.5420\n",
      "Epoch [2/10], Step [200/500] Loss: 2.4997\n",
      "Epoch [2/10], Step [300/500] Loss: 2.5480\n",
      "Epoch [2/10], Step [400/500] Loss: 2.4075\n",
      "Epoch [2/10], Step [500/500] Loss: 2.5638\n",
      "Current learning rate:  0.001\n",
      "Epoch [3/10], Step [100/500] Loss: 2.5754\n",
      "Epoch [3/10], Step [200/500] Loss: 2.4558\n",
      "Epoch [3/10], Step [300/500] Loss: 2.4161\n",
      "Epoch [3/10], Step [400/500] Loss: 2.4186\n",
      "Epoch [3/10], Step [500/500] Loss: 2.5052\n",
      "Current learning rate:  0.001\n",
      "Epoch [4/10], Step [100/500] Loss: 2.4800\n",
      "Epoch [4/10], Step [200/500] Loss: 2.6370\n",
      "Epoch [4/10], Step [300/500] Loss: 2.5812\n",
      "Epoch [4/10], Step [400/500] Loss: 2.5622\n",
      "Epoch [4/10], Step [500/500] Loss: 2.5759\n",
      "Current learning rate:  0.001\n",
      "Epoch [5/10], Step [100/500] Loss: 2.4078\n",
      "Epoch [5/10], Step [200/500] Loss: 2.5893\n",
      "Epoch [5/10], Step [300/500] Loss: 2.5185\n",
      "Epoch [5/10], Step [400/500] Loss: 2.5792\n",
      "Epoch [5/10], Step [500/500] Loss: 2.4982\n",
      "Current learning rate:  0.001\n",
      "Epoch [6/10], Step [100/500] Loss: 2.4735\n",
      "Epoch [6/10], Step [200/500] Loss: 2.5192\n",
      "Epoch [6/10], Step [300/500] Loss: 2.4522\n",
      "Epoch [6/10], Step [400/500] Loss: 2.6781\n",
      "Epoch [6/10], Step [500/500] Loss: 2.4799\n",
      "Current learning rate:  0.001\n",
      "Epoch [7/10], Step [100/500] Loss: 2.5511\n",
      "Epoch [7/10], Step [200/500] Loss: 2.4071\n",
      "Epoch [7/10], Step [300/500] Loss: 2.6033\n",
      "Epoch [7/10], Step [400/500] Loss: 2.4527\n",
      "Epoch [7/10], Step [500/500] Loss: 2.4955\n",
      "Current learning rate:  0.001\n",
      "Epoch [8/10], Step [100/500] Loss: 2.5299\n",
      "Epoch [8/10], Step [200/500] Loss: 2.4033\n",
      "Epoch [8/10], Step [300/500] Loss: 2.5478\n",
      "Epoch [8/10], Step [400/500] Loss: 2.5164\n",
      "Epoch [8/10], Step [500/500] Loss: 2.6480\n",
      "Current learning rate:  0.001\n",
      "Epoch [9/10], Step [100/500] Loss: 2.5238\n",
      "Epoch [9/10], Step [200/500] Loss: 2.4097\n",
      "Epoch [9/10], Step [300/500] Loss: 2.6056\n",
      "Epoch [9/10], Step [400/500] Loss: 2.5536\n",
      "Epoch [9/10], Step [500/500] Loss: 2.4874\n",
      "Current learning rate:  0.001\n",
      "Epoch [10/10], Step [100/500] Loss: 2.4875\n",
      "Epoch [10/10], Step [200/500] Loss: 2.5047\n",
      "Epoch [10/10], Step [300/500] Loss: 2.5384\n",
      "Epoch [10/10], Step [400/500] Loss: 2.5440\n",
      "Epoch [10/10], Step [500/500] Loss: 2.5508\n",
      "Current learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "model2 = train_model(train_loader, model2, num_epochs, error, optimizer, curr_lr, total_step, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Epoch [1/80], Step [100/500] Loss: 2.6815\n",
      "Epoch [1/80], Step [200/500] Loss: 2.8038\n",
      "Epoch [1/80], Step [300/500] Loss: 2.8482\n",
      "Epoch [1/80], Step [400/500] Loss: 2.8780\n",
      "Epoch [1/80], Step [500/500] Loss: 2.7598\n",
      "Current learning rate:  0.01\n",
      "Epoch [2/80], Step [100/500] Loss: 2.7672\n",
      "Epoch [2/80], Step [200/500] Loss: 2.9197\n",
      "Epoch [2/80], Step [300/500] Loss: 2.9226\n",
      "Epoch [2/80], Step [400/500] Loss: 2.7696\n",
      "Epoch [2/80], Step [500/500] Loss: 2.6829\n",
      "Current learning rate:  0.01\n",
      "Epoch [3/80], Step [100/500] Loss: 2.7945\n",
      "Epoch [3/80], Step [200/500] Loss: 2.7920\n",
      "Epoch [3/80], Step [300/500] Loss: 2.7224\n",
      "Epoch [3/80], Step [400/500] Loss: 2.9192\n",
      "Epoch [3/80], Step [500/500] Loss: 2.7877\n",
      "Current learning rate:  0.01\n",
      "Epoch [4/80], Step [100/500] Loss: 2.8258\n",
      "Epoch [4/80], Step [200/500] Loss: 2.9351\n",
      "Epoch [4/80], Step [300/500] Loss: 2.7965\n",
      "Epoch [4/80], Step [400/500] Loss: 2.7749\n",
      "Epoch [4/80], Step [500/500] Loss: 2.7781\n",
      "Current learning rate:  0.01\n",
      "Epoch [5/80], Step [100/500] Loss: 2.8633\n",
      "Epoch [5/80], Step [200/500] Loss: 2.8506\n",
      "Epoch [5/80], Step [300/500] Loss: 2.9326\n",
      "Epoch [5/80], Step [400/500] Loss: 2.9293\n",
      "Epoch [5/80], Step [500/500] Loss: 2.6413\n",
      "Current learning rate:  0.01\n",
      "Epoch [6/80], Step [100/500] Loss: 2.7744\n",
      "Epoch [6/80], Step [200/500] Loss: 2.8867\n",
      "Epoch [6/80], Step [300/500] Loss: 2.8392\n",
      "Epoch [6/80], Step [400/500] Loss: 2.8102\n",
      "Epoch [6/80], Step [500/500] Loss: 2.9252\n",
      "Current learning rate:  0.01\n",
      "Epoch [7/80], Step [100/500] Loss: 2.9841\n",
      "Epoch [7/80], Step [200/500] Loss: 2.9363\n",
      "Epoch [7/80], Step [300/500] Loss: 2.8347\n",
      "Epoch [7/80], Step [400/500] Loss: 2.8261\n",
      "Epoch [7/80], Step [500/500] Loss: 2.8777\n",
      "Current learning rate:  0.01\n",
      "Epoch [8/80], Step [100/500] Loss: 2.9420\n",
      "Epoch [8/80], Step [200/500] Loss: 2.8670\n",
      "Epoch [8/80], Step [300/500] Loss: 2.9576\n",
      "Epoch [8/80], Step [400/500] Loss: 2.8332\n",
      "Epoch [8/80], Step [500/500] Loss: 2.9249\n",
      "Current learning rate:  0.01\n",
      "Epoch [9/80], Step [100/500] Loss: 2.7881\n",
      "Epoch [9/80], Step [200/500] Loss: 2.9094\n",
      "Epoch [9/80], Step [300/500] Loss: 2.7095\n",
      "Epoch [9/80], Step [400/500] Loss: 2.8362\n",
      "Epoch [9/80], Step [500/500] Loss: 2.8272\n",
      "Current learning rate:  0.01\n",
      "Epoch [10/80], Step [100/500] Loss: 2.7984\n",
      "Epoch [10/80], Step [200/500] Loss: 2.8233\n",
      "Epoch [10/80], Step [300/500] Loss: 2.7945\n",
      "Epoch [10/80], Step [400/500] Loss: 2.7173\n",
      "Epoch [10/80], Step [500/500] Loss: 2.7274\n",
      "Current learning rate:  0.01\n",
      "Epoch [11/80], Step [100/500] Loss: 2.8679\n",
      "Epoch [11/80], Step [200/500] Loss: 2.8427\n",
      "Epoch [11/80], Step [300/500] Loss: 2.8240\n",
      "Epoch [11/80], Step [400/500] Loss: 2.8166\n",
      "Epoch [11/80], Step [500/500] Loss: 2.6620\n",
      "Current learning rate:  0.01\n",
      "Epoch [12/80], Step [100/500] Loss: 2.8181\n",
      "Epoch [12/80], Step [200/500] Loss: 3.0186\n",
      "Epoch [12/80], Step [300/500] Loss: 2.6016\n",
      "Epoch [12/80], Step [400/500] Loss: 2.7035\n",
      "Epoch [12/80], Step [500/500] Loss: 2.8045\n",
      "Current learning rate:  0.01\n",
      "Epoch [13/80], Step [100/500] Loss: 2.7586\n",
      "Epoch [13/80], Step [200/500] Loss: 2.8709\n",
      "Epoch [13/80], Step [300/500] Loss: 2.7973\n",
      "Epoch [13/80], Step [400/500] Loss: 2.9715\n",
      "Epoch [13/80], Step [500/500] Loss: 2.7610\n",
      "Current learning rate:  0.01\n",
      "Epoch [14/80], Step [100/500] Loss: 2.8593\n",
      "Epoch [14/80], Step [200/500] Loss: 2.7853\n",
      "Epoch [14/80], Step [300/500] Loss: 2.7258\n",
      "Epoch [14/80], Step [400/500] Loss: 2.7823\n",
      "Epoch [14/80], Step [500/500] Loss: 2.7754\n",
      "Current learning rate:  0.01\n",
      "Epoch [15/80], Step [100/500] Loss: 2.8642\n",
      "Epoch [15/80], Step [200/500] Loss: 2.7700\n",
      "Epoch [15/80], Step [300/500] Loss: 2.7202\n",
      "Epoch [15/80], Step [400/500] Loss: 2.6957\n",
      "Epoch [15/80], Step [500/500] Loss: 2.8790\n",
      "Current learning rate:  0.01\n",
      "Epoch [16/80], Step [100/500] Loss: 2.9630\n",
      "Epoch [16/80], Step [200/500] Loss: 2.7646\n",
      "Epoch [16/80], Step [300/500] Loss: 2.8335\n",
      "Epoch [16/80], Step [400/500] Loss: 2.7860\n",
      "Epoch [16/80], Step [500/500] Loss: 2.8081\n",
      "Current learning rate:  0.01\n",
      "Epoch [17/80], Step [100/500] Loss: 2.7725\n",
      "Epoch [17/80], Step [200/500] Loss: 2.9326\n",
      "Epoch [17/80], Step [300/500] Loss: 2.8435\n",
      "Epoch [17/80], Step [400/500] Loss: 2.6440\n",
      "Epoch [17/80], Step [500/500] Loss: 2.8455\n",
      "Current learning rate:  0.01\n",
      "Epoch [18/80], Step [100/500] Loss: 2.8572\n",
      "Epoch [18/80], Step [200/500] Loss: 2.8258\n",
      "Epoch [18/80], Step [300/500] Loss: 2.9803\n",
      "Epoch [18/80], Step [400/500] Loss: 2.8189\n",
      "Epoch [18/80], Step [500/500] Loss: 2.7355\n",
      "Current learning rate:  0.01\n",
      "Epoch [19/80], Step [100/500] Loss: 2.7316\n",
      "Epoch [19/80], Step [200/500] Loss: 2.9000\n",
      "Epoch [19/80], Step [300/500] Loss: 2.8966\n",
      "Epoch [19/80], Step [400/500] Loss: 2.8812\n",
      "Epoch [19/80], Step [500/500] Loss: 2.7930\n",
      "Current learning rate:  0.01\n",
      "Epoch [20/80], Step [100/500] Loss: 2.9648\n",
      "Epoch [20/80], Step [200/500] Loss: 2.8596\n",
      "Epoch [20/80], Step [300/500] Loss: 2.7554\n",
      "Epoch [20/80], Step [400/500] Loss: 2.9102\n",
      "Epoch [20/80], Step [500/500] Loss: 2.7857\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [21/80], Step [100/500] Loss: 2.6523\n",
      "Epoch [21/80], Step [200/500] Loss: 2.7687\n",
      "Epoch [21/80], Step [300/500] Loss: 2.7004\n",
      "Epoch [21/80], Step [400/500] Loss: 2.7672\n",
      "Epoch [21/80], Step [500/500] Loss: 2.7925\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [22/80], Step [100/500] Loss: 2.8262\n",
      "Epoch [22/80], Step [200/500] Loss: 3.0268\n",
      "Epoch [22/80], Step [300/500] Loss: 2.8653\n",
      "Epoch [22/80], Step [400/500] Loss: 2.8076\n",
      "Epoch [22/80], Step [500/500] Loss: 2.8589\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [23/80], Step [100/500] Loss: 2.7588\n",
      "Epoch [23/80], Step [200/500] Loss: 2.9343\n",
      "Epoch [23/80], Step [300/500] Loss: 2.8852\n",
      "Epoch [23/80], Step [400/500] Loss: 2.7676\n",
      "Epoch [23/80], Step [500/500] Loss: 2.8477\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [24/80], Step [100/500] Loss: 2.7587\n",
      "Epoch [24/80], Step [200/500] Loss: 2.8145\n",
      "Epoch [24/80], Step [300/500] Loss: 2.9316\n",
      "Epoch [24/80], Step [400/500] Loss: 2.9149\n",
      "Epoch [24/80], Step [500/500] Loss: 2.9053\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [25/80], Step [100/500] Loss: 2.6256\n",
      "Epoch [25/80], Step [200/500] Loss: 2.9724\n",
      "Epoch [25/80], Step [300/500] Loss: 2.9453\n",
      "Epoch [25/80], Step [400/500] Loss: 2.9099\n",
      "Epoch [25/80], Step [500/500] Loss: 2.6775\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [26/80], Step [100/500] Loss: 2.7735\n",
      "Epoch [26/80], Step [200/500] Loss: 2.7768\n",
      "Epoch [26/80], Step [300/500] Loss: 3.0467\n",
      "Epoch [26/80], Step [400/500] Loss: 2.6501\n",
      "Epoch [26/80], Step [500/500] Loss: 2.8033\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [27/80], Step [100/500] Loss: 2.8622\n",
      "Epoch [27/80], Step [200/500] Loss: 2.8268\n",
      "Epoch [27/80], Step [300/500] Loss: 2.8449\n",
      "Epoch [27/80], Step [400/500] Loss: 2.7893\n",
      "Epoch [27/80], Step [500/500] Loss: 2.8146\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [28/80], Step [100/500] Loss: 2.8908\n",
      "Epoch [28/80], Step [200/500] Loss: 2.9157\n",
      "Epoch [28/80], Step [300/500] Loss: 2.6892\n",
      "Epoch [28/80], Step [400/500] Loss: 2.7925\n",
      "Epoch [28/80], Step [500/500] Loss: 3.0059\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [29/80], Step [100/500] Loss: 2.9000\n",
      "Epoch [29/80], Step [200/500] Loss: 2.8687\n",
      "Epoch [29/80], Step [300/500] Loss: 2.8611\n",
      "Epoch [29/80], Step [400/500] Loss: 2.8698\n",
      "Epoch [29/80], Step [500/500] Loss: 2.9713\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [30/80], Step [100/500] Loss: 2.8043\n",
      "Epoch [30/80], Step [200/500] Loss: 2.7249\n",
      "Epoch [30/80], Step [300/500] Loss: 2.6688\n",
      "Epoch [30/80], Step [400/500] Loss: 2.9255\n",
      "Epoch [30/80], Step [500/500] Loss: 2.7444\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [31/80], Step [100/500] Loss: 2.8324\n",
      "Epoch [31/80], Step [200/500] Loss: 2.7806\n",
      "Epoch [31/80], Step [300/500] Loss: 2.9090\n",
      "Epoch [31/80], Step [400/500] Loss: 2.9512\n",
      "Epoch [31/80], Step [500/500] Loss: 2.8125\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [32/80], Step [100/500] Loss: 2.7737\n",
      "Epoch [32/80], Step [200/500] Loss: 2.7926\n",
      "Epoch [32/80], Step [300/500] Loss: 2.7391\n",
      "Epoch [32/80], Step [400/500] Loss: 2.6826\n",
      "Epoch [32/80], Step [500/500] Loss: 2.8298\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [33/80], Step [100/500] Loss: 2.9030\n",
      "Epoch [33/80], Step [200/500] Loss: 2.8724\n",
      "Epoch [33/80], Step [300/500] Loss: 2.8917\n",
      "Epoch [33/80], Step [400/500] Loss: 2.9203\n",
      "Epoch [33/80], Step [500/500] Loss: 2.8272\n",
      "Current learning rate:  0.0033333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/80], Step [100/500] Loss: 2.8145\n",
      "Epoch [34/80], Step [200/500] Loss: 2.7828\n",
      "Epoch [34/80], Step [300/500] Loss: 2.6384\n",
      "Epoch [34/80], Step [400/500] Loss: 2.8361\n",
      "Epoch [34/80], Step [500/500] Loss: 2.7623\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [35/80], Step [100/500] Loss: 2.6467\n",
      "Epoch [35/80], Step [200/500] Loss: 2.5736\n",
      "Epoch [35/80], Step [300/500] Loss: 2.8314\n",
      "Epoch [35/80], Step [400/500] Loss: 2.8587\n",
      "Epoch [35/80], Step [500/500] Loss: 3.0090\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [36/80], Step [100/500] Loss: 2.9356\n",
      "Epoch [36/80], Step [200/500] Loss: 2.8161\n",
      "Epoch [36/80], Step [300/500] Loss: 2.8004\n",
      "Epoch [36/80], Step [400/500] Loss: 2.7757\n",
      "Epoch [36/80], Step [500/500] Loss: 2.8072\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [37/80], Step [100/500] Loss: 2.6152\n",
      "Epoch [37/80], Step [200/500] Loss: 2.8019\n",
      "Epoch [37/80], Step [300/500] Loss: 2.7942\n",
      "Epoch [37/80], Step [400/500] Loss: 2.8023\n",
      "Epoch [37/80], Step [500/500] Loss: 2.9453\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [38/80], Step [100/500] Loss: 2.7916\n",
      "Epoch [38/80], Step [200/500] Loss: 2.8213\n",
      "Epoch [38/80], Step [300/500] Loss: 2.6903\n",
      "Epoch [38/80], Step [400/500] Loss: 2.8334\n",
      "Epoch [38/80], Step [500/500] Loss: 2.8259\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [39/80], Step [100/500] Loss: 2.9888\n",
      "Epoch [39/80], Step [200/500] Loss: 2.7207\n",
      "Epoch [39/80], Step [300/500] Loss: 2.8598\n",
      "Epoch [39/80], Step [400/500] Loss: 2.9887\n",
      "Epoch [39/80], Step [500/500] Loss: 2.7547\n",
      "Current learning rate:  0.0033333333333333335\n",
      "Epoch [40/80], Step [100/500] Loss: 2.8278\n",
      "Epoch [40/80], Step [200/500] Loss: 2.7434\n",
      "Epoch [40/80], Step [300/500] Loss: 2.8575\n",
      "Epoch [40/80], Step [400/500] Loss: 2.7320\n",
      "Epoch [40/80], Step [500/500] Loss: 3.0072\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [41/80], Step [100/500] Loss: 2.8399\n",
      "Epoch [41/80], Step [200/500] Loss: 2.8176\n",
      "Epoch [41/80], Step [300/500] Loss: 2.9564\n",
      "Epoch [41/80], Step [400/500] Loss: 2.8051\n",
      "Epoch [41/80], Step [500/500] Loss: 2.8104\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [42/80], Step [100/500] Loss: 2.8599\n",
      "Epoch [42/80], Step [200/500] Loss: 2.7824\n",
      "Epoch [42/80], Step [300/500] Loss: 2.7748\n",
      "Epoch [42/80], Step [400/500] Loss: 2.7842\n",
      "Epoch [42/80], Step [500/500] Loss: 3.0010\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [43/80], Step [100/500] Loss: 2.7642\n",
      "Epoch [43/80], Step [200/500] Loss: 2.8266\n",
      "Epoch [43/80], Step [300/500] Loss: 2.6135\n",
      "Epoch [43/80], Step [400/500] Loss: 2.8594\n",
      "Epoch [43/80], Step [500/500] Loss: 2.9195\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [44/80], Step [100/500] Loss: 2.9096\n",
      "Epoch [44/80], Step [200/500] Loss: 2.8728\n",
      "Epoch [44/80], Step [300/500] Loss: 2.7814\n",
      "Epoch [44/80], Step [400/500] Loss: 2.6298\n",
      "Epoch [44/80], Step [500/500] Loss: 2.7993\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [45/80], Step [100/500] Loss: 2.7967\n",
      "Epoch [45/80], Step [200/500] Loss: 2.8267\n",
      "Epoch [45/80], Step [300/500] Loss: 2.8680\n",
      "Epoch [45/80], Step [400/500] Loss: 2.7837\n",
      "Epoch [45/80], Step [500/500] Loss: 2.8660\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [46/80], Step [100/500] Loss: 2.8841\n",
      "Epoch [46/80], Step [200/500] Loss: 2.6999\n",
      "Epoch [46/80], Step [300/500] Loss: 2.6945\n",
      "Epoch [46/80], Step [400/500] Loss: 2.9000\n",
      "Epoch [46/80], Step [500/500] Loss: 2.9238\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [47/80], Step [100/500] Loss: 2.8032\n",
      "Epoch [47/80], Step [200/500] Loss: 2.8580\n",
      "Epoch [47/80], Step [300/500] Loss: 2.8831\n",
      "Epoch [47/80], Step [400/500] Loss: 2.9277\n",
      "Epoch [47/80], Step [500/500] Loss: 2.8240\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [48/80], Step [100/500] Loss: 2.6100\n",
      "Epoch [48/80], Step [200/500] Loss: 2.7080\n",
      "Epoch [48/80], Step [300/500] Loss: 2.7522\n",
      "Epoch [48/80], Step [400/500] Loss: 2.9380\n",
      "Epoch [48/80], Step [500/500] Loss: 2.7668\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [49/80], Step [100/500] Loss: 2.8586\n",
      "Epoch [49/80], Step [200/500] Loss: 2.6976\n",
      "Epoch [49/80], Step [300/500] Loss: 2.9323\n",
      "Epoch [49/80], Step [400/500] Loss: 2.7796\n",
      "Epoch [49/80], Step [500/500] Loss: 2.8945\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [50/80], Step [100/500] Loss: 2.9233\n",
      "Epoch [50/80], Step [200/500] Loss: 2.7411\n",
      "Epoch [50/80], Step [300/500] Loss: 2.9140\n",
      "Epoch [50/80], Step [400/500] Loss: 2.5933\n",
      "Epoch [50/80], Step [500/500] Loss: 2.8380\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [51/80], Step [100/500] Loss: 2.7412\n",
      "Epoch [51/80], Step [200/500] Loss: 2.9823\n",
      "Epoch [51/80], Step [300/500] Loss: 2.9907\n",
      "Epoch [51/80], Step [400/500] Loss: 3.0281\n",
      "Epoch [51/80], Step [500/500] Loss: 2.7638\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [52/80], Step [100/500] Loss: 2.6644\n",
      "Epoch [52/80], Step [200/500] Loss: 2.8322\n",
      "Epoch [52/80], Step [300/500] Loss: 2.7479\n",
      "Epoch [52/80], Step [400/500] Loss: 2.8878\n",
      "Epoch [52/80], Step [500/500] Loss: 2.7071\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [53/80], Step [100/500] Loss: 2.8914\n",
      "Epoch [53/80], Step [200/500] Loss: 2.8066\n",
      "Epoch [53/80], Step [300/500] Loss: 2.8018\n",
      "Epoch [53/80], Step [400/500] Loss: 2.7883\n",
      "Epoch [53/80], Step [500/500] Loss: 2.8189\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [54/80], Step [100/500] Loss: 2.6792\n",
      "Epoch [54/80], Step [200/500] Loss: 2.9143\n",
      "Epoch [54/80], Step [300/500] Loss: 2.7861\n",
      "Epoch [54/80], Step [400/500] Loss: 2.7807\n",
      "Epoch [54/80], Step [500/500] Loss: 2.8762\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [55/80], Step [100/500] Loss: 2.8416\n",
      "Epoch [55/80], Step [200/500] Loss: 2.6798\n",
      "Epoch [55/80], Step [300/500] Loss: 2.8386\n",
      "Epoch [55/80], Step [400/500] Loss: 2.6660\n",
      "Epoch [55/80], Step [500/500] Loss: 2.8046\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [56/80], Step [100/500] Loss: 2.8657\n",
      "Epoch [56/80], Step [200/500] Loss: 2.6734\n",
      "Epoch [56/80], Step [300/500] Loss: 2.8897\n",
      "Epoch [56/80], Step [400/500] Loss: 2.8150\n",
      "Epoch [56/80], Step [500/500] Loss: 2.7934\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [57/80], Step [100/500] Loss: 2.8137\n",
      "Epoch [57/80], Step [200/500] Loss: 2.9111\n",
      "Epoch [57/80], Step [300/500] Loss: 2.5970\n",
      "Epoch [57/80], Step [400/500] Loss: 2.7864\n",
      "Epoch [57/80], Step [500/500] Loss: 2.7419\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [58/80], Step [100/500] Loss: 2.8364\n",
      "Epoch [58/80], Step [200/500] Loss: 2.8535\n",
      "Epoch [58/80], Step [300/500] Loss: 2.9444\n",
      "Epoch [58/80], Step [400/500] Loss: 2.6989\n",
      "Epoch [58/80], Step [500/500] Loss: 2.8687\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [59/80], Step [100/500] Loss: 2.8652\n",
      "Epoch [59/80], Step [200/500] Loss: 2.8946\n",
      "Epoch [59/80], Step [300/500] Loss: 2.9199\n",
      "Epoch [59/80], Step [400/500] Loss: 3.0123\n",
      "Epoch [59/80], Step [500/500] Loss: 2.5414\n",
      "Current learning rate:  0.0011111111111111111\n",
      "Epoch [60/80], Step [100/500] Loss: 2.7579\n",
      "Epoch [60/80], Step [200/500] Loss: 2.8201\n",
      "Epoch [60/80], Step [300/500] Loss: 2.8311\n",
      "Epoch [60/80], Step [400/500] Loss: 2.7898\n",
      "Epoch [60/80], Step [500/500] Loss: 2.8861\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [61/80], Step [100/500] Loss: 2.9801\n",
      "Epoch [61/80], Step [200/500] Loss: 2.9413\n",
      "Epoch [61/80], Step [300/500] Loss: 2.6771\n",
      "Epoch [61/80], Step [400/500] Loss: 2.8990\n",
      "Epoch [61/80], Step [500/500] Loss: 2.8778\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [62/80], Step [100/500] Loss: 2.8179\n",
      "Epoch [62/80], Step [200/500] Loss: 2.7676\n",
      "Epoch [62/80], Step [300/500] Loss: 2.7523\n",
      "Epoch [62/80], Step [400/500] Loss: 2.8786\n",
      "Epoch [62/80], Step [500/500] Loss: 2.8872\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [63/80], Step [100/500] Loss: 2.8314\n",
      "Epoch [63/80], Step [200/500] Loss: 2.8047\n",
      "Epoch [63/80], Step [300/500] Loss: 2.8564\n",
      "Epoch [63/80], Step [400/500] Loss: 2.8025\n",
      "Epoch [63/80], Step [500/500] Loss: 2.8006\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [64/80], Step [100/500] Loss: 2.6767\n",
      "Epoch [64/80], Step [200/500] Loss: 2.8242\n",
      "Epoch [64/80], Step [300/500] Loss: 2.6541\n",
      "Epoch [64/80], Step [400/500] Loss: 2.9252\n",
      "Epoch [64/80], Step [500/500] Loss: 2.7975\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [65/80], Step [100/500] Loss: 2.6765\n",
      "Epoch [65/80], Step [200/500] Loss: 2.7077\n",
      "Epoch [65/80], Step [300/500] Loss: 2.9773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/80], Step [400/500] Loss: 2.7568\n",
      "Epoch [65/80], Step [500/500] Loss: 2.7690\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [66/80], Step [100/500] Loss: 2.8885\n",
      "Epoch [66/80], Step [200/500] Loss: 2.6624\n",
      "Epoch [66/80], Step [300/500] Loss: 2.8301\n",
      "Epoch [66/80], Step [400/500] Loss: 2.6889\n",
      "Epoch [66/80], Step [500/500] Loss: 2.9316\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [67/80], Step [100/500] Loss: 2.6524\n",
      "Epoch [67/80], Step [200/500] Loss: 2.8228\n",
      "Epoch [67/80], Step [300/500] Loss: 2.7594\n",
      "Epoch [67/80], Step [400/500] Loss: 2.8792\n",
      "Epoch [67/80], Step [500/500] Loss: 2.8735\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [68/80], Step [100/500] Loss: 2.7397\n",
      "Epoch [68/80], Step [200/500] Loss: 2.6089\n",
      "Epoch [68/80], Step [300/500] Loss: 2.8961\n",
      "Epoch [68/80], Step [400/500] Loss: 2.8936\n",
      "Epoch [68/80], Step [500/500] Loss: 2.8848\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [69/80], Step [100/500] Loss: 2.8813\n",
      "Epoch [69/80], Step [200/500] Loss: 2.8944\n",
      "Epoch [69/80], Step [300/500] Loss: 2.7682\n",
      "Epoch [69/80], Step [400/500] Loss: 2.9819\n",
      "Epoch [69/80], Step [500/500] Loss: 2.6961\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [70/80], Step [100/500] Loss: 2.7875\n",
      "Epoch [70/80], Step [200/500] Loss: 2.8052\n",
      "Epoch [70/80], Step [300/500] Loss: 2.7991\n",
      "Epoch [70/80], Step [400/500] Loss: 2.9363\n",
      "Epoch [70/80], Step [500/500] Loss: 2.6532\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [71/80], Step [100/500] Loss: 2.9122\n",
      "Epoch [71/80], Step [200/500] Loss: 2.8095\n",
      "Epoch [71/80], Step [300/500] Loss: 2.6921\n",
      "Epoch [71/80], Step [400/500] Loss: 2.7920\n",
      "Epoch [71/80], Step [500/500] Loss: 2.8292\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [72/80], Step [100/500] Loss: 2.7816\n",
      "Epoch [72/80], Step [200/500] Loss: 2.8243\n",
      "Epoch [72/80], Step [300/500] Loss: 2.8789\n",
      "Epoch [72/80], Step [400/500] Loss: 2.9242\n",
      "Epoch [72/80], Step [500/500] Loss: 2.7419\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [73/80], Step [100/500] Loss: 2.8956\n",
      "Epoch [73/80], Step [200/500] Loss: 2.8854\n",
      "Epoch [73/80], Step [300/500] Loss: 2.8121\n",
      "Epoch [73/80], Step [400/500] Loss: 2.8979\n",
      "Epoch [73/80], Step [500/500] Loss: 2.7115\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [74/80], Step [100/500] Loss: 2.8135\n",
      "Epoch [74/80], Step [200/500] Loss: 2.8079\n",
      "Epoch [74/80], Step [300/500] Loss: 2.7977\n",
      "Epoch [74/80], Step [400/500] Loss: 2.8141\n",
      "Epoch [74/80], Step [500/500] Loss: 2.8049\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [75/80], Step [100/500] Loss: 2.7752\n",
      "Epoch [75/80], Step [200/500] Loss: 2.7223\n",
      "Epoch [75/80], Step [300/500] Loss: 2.9273\n",
      "Epoch [75/80], Step [400/500] Loss: 2.7506\n",
      "Epoch [75/80], Step [500/500] Loss: 2.7385\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [76/80], Step [100/500] Loss: 2.7236\n",
      "Epoch [76/80], Step [200/500] Loss: 2.8728\n",
      "Epoch [76/80], Step [300/500] Loss: 2.8695\n",
      "Epoch [76/80], Step [400/500] Loss: 2.7379\n",
      "Epoch [76/80], Step [500/500] Loss: 2.8028\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [77/80], Step [100/500] Loss: 2.7380\n",
      "Epoch [77/80], Step [200/500] Loss: 2.8253\n",
      "Epoch [77/80], Step [300/500] Loss: 2.7672\n",
      "Epoch [77/80], Step [400/500] Loss: 2.7899\n",
      "Epoch [77/80], Step [500/500] Loss: 2.6654\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [78/80], Step [100/500] Loss: 2.8142\n",
      "Epoch [78/80], Step [200/500] Loss: 2.7420\n",
      "Epoch [78/80], Step [300/500] Loss: 2.9314\n",
      "Epoch [78/80], Step [400/500] Loss: 2.8045\n",
      "Epoch [78/80], Step [500/500] Loss: 2.8714\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [79/80], Step [100/500] Loss: 2.8598\n",
      "Epoch [79/80], Step [200/500] Loss: 2.9611\n",
      "Epoch [79/80], Step [300/500] Loss: 2.6871\n",
      "Epoch [79/80], Step [400/500] Loss: 2.9111\n",
      "Epoch [79/80], Step [500/500] Loss: 2.8339\n",
      "Current learning rate:  0.00037037037037037035\n",
      "Epoch [80/80], Step [100/500] Loss: 2.7476\n",
      "Epoch [80/80], Step [200/500] Loss: 2.7531\n",
      "Epoch [80/80], Step [300/500] Loss: 2.8660\n",
      "Epoch [80/80], Step [400/500] Loss: 2.6673\n",
      "Epoch [80/80], Step [500/500] Loss: 2.8532\n",
      "Current learning rate:  0.0001234567901234568\n"
     ]
    }
   ],
   "source": [
    "model3 = train_model(train_loader, model3, 80, error, optimizer, 0.01, total_step, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Epoch [1/50], Step [100/500] Loss: 2.8199\n",
      "Epoch [1/50], Step [200/500] Loss: 2.7521\n",
      "Epoch [1/50], Step [300/500] Loss: 2.6919\n",
      "Epoch [1/50], Step [400/500] Loss: 2.8673\n",
      "Epoch [1/50], Step [500/500] Loss: 2.6657\n",
      "Current learning rate:  0.001\n",
      "Epoch [2/50], Step [100/500] Loss: 2.6928\n",
      "Epoch [2/50], Step [200/500] Loss: 2.6839\n",
      "Epoch [2/50], Step [300/500] Loss: 2.6584\n",
      "Epoch [2/50], Step [400/500] Loss: 2.7283\n",
      "Epoch [2/50], Step [500/500] Loss: 2.7093\n",
      "Current learning rate:  0.001\n",
      "Epoch [3/50], Step [100/500] Loss: 2.7731\n",
      "Epoch [3/50], Step [200/500] Loss: 2.8198\n",
      "Epoch [3/50], Step [300/500] Loss: 2.7120\n",
      "Epoch [3/50], Step [400/500] Loss: 2.8589\n",
      "Epoch [3/50], Step [500/500] Loss: 2.5999\n",
      "Current learning rate:  0.001\n",
      "Epoch [4/50], Step [100/500] Loss: 2.8063\n",
      "Epoch [4/50], Step [200/500] Loss: 2.7485\n",
      "Epoch [4/50], Step [300/500] Loss: 2.8649\n",
      "Epoch [4/50], Step [400/500] Loss: 2.8521\n",
      "Epoch [4/50], Step [500/500] Loss: 2.8000\n",
      "Current learning rate:  0.001\n",
      "Epoch [5/50], Step [100/500] Loss: 2.8208\n",
      "Epoch [5/50], Step [200/500] Loss: 2.7702\n",
      "Epoch [5/50], Step [300/500] Loss: 2.8253\n",
      "Epoch [5/50], Step [400/500] Loss: 2.7946\n",
      "Epoch [5/50], Step [500/500] Loss: 2.6516\n",
      "Current learning rate:  0.001\n",
      "Epoch [6/50], Step [100/500] Loss: 2.7492\n",
      "Epoch [6/50], Step [200/500] Loss: 2.8752\n",
      "Epoch [6/50], Step [300/500] Loss: 2.6615\n",
      "Epoch [6/50], Step [400/500] Loss: 2.7879\n",
      "Epoch [6/50], Step [500/500] Loss: 2.7740\n",
      "Current learning rate:  0.001\n",
      "Epoch [7/50], Step [100/500] Loss: 2.7721\n",
      "Epoch [7/50], Step [200/500] Loss: 2.7308\n",
      "Epoch [7/50], Step [300/500] Loss: 2.8030\n",
      "Epoch [7/50], Step [400/500] Loss: 2.7836\n",
      "Epoch [7/50], Step [500/500] Loss: 2.7679\n",
      "Current learning rate:  0.001\n",
      "Epoch [8/50], Step [100/500] Loss: 2.8339\n",
      "Epoch [8/50], Step [200/500] Loss: 2.8811\n",
      "Epoch [8/50], Step [300/500] Loss: 2.8756\n",
      "Epoch [8/50], Step [400/500] Loss: 2.6376\n",
      "Epoch [8/50], Step [500/500] Loss: 2.7489\n",
      "Current learning rate:  0.001\n",
      "Epoch [9/50], Step [100/500] Loss: 2.7963\n",
      "Epoch [9/50], Step [200/500] Loss: 2.7435\n",
      "Epoch [9/50], Step [300/500] Loss: 2.8117\n",
      "Epoch [9/50], Step [400/500] Loss: 2.5862\n",
      "Epoch [9/50], Step [500/500] Loss: 2.6979\n",
      "Current learning rate:  0.001\n",
      "Epoch [10/50], Step [100/500] Loss: 2.7771\n",
      "Epoch [10/50], Step [200/500] Loss: 2.6805\n",
      "Epoch [10/50], Step [300/500] Loss: 2.7587\n",
      "Epoch [10/50], Step [400/500] Loss: 2.8190\n",
      "Epoch [10/50], Step [500/500] Loss: 2.8312\n",
      "Current learning rate:  0.001\n",
      "Epoch [11/50], Step [100/500] Loss: 2.5025\n",
      "Epoch [11/50], Step [200/500] Loss: 2.7698\n",
      "Epoch [11/50], Step [300/500] Loss: 2.7983\n",
      "Epoch [11/50], Step [400/500] Loss: 2.7909\n",
      "Epoch [11/50], Step [500/500] Loss: 2.7901\n",
      "Current learning rate:  0.001\n",
      "Epoch [12/50], Step [100/500] Loss: 2.7632\n",
      "Epoch [12/50], Step [200/500] Loss: 2.7621\n",
      "Epoch [12/50], Step [300/500] Loss: 2.7359\n",
      "Epoch [12/50], Step [400/500] Loss: 2.8530\n",
      "Epoch [12/50], Step [500/500] Loss: 2.8220\n",
      "Current learning rate:  0.001\n",
      "Epoch [13/50], Step [100/500] Loss: 2.9163\n",
      "Epoch [13/50], Step [200/500] Loss: 2.7865\n",
      "Epoch [13/50], Step [300/500] Loss: 2.7266\n",
      "Epoch [13/50], Step [400/500] Loss: 2.5840\n",
      "Epoch [13/50], Step [500/500] Loss: 2.8104\n",
      "Current learning rate:  0.001\n",
      "Epoch [14/50], Step [100/500] Loss: 2.7215\n",
      "Epoch [14/50], Step [200/500] Loss: 2.6345\n",
      "Epoch [14/50], Step [300/500] Loss: 2.9608\n",
      "Epoch [14/50], Step [400/500] Loss: 2.7541\n",
      "Epoch [14/50], Step [500/500] Loss: 2.5725\n",
      "Current learning rate:  0.001\n",
      "Epoch [15/50], Step [100/500] Loss: 2.8692\n",
      "Epoch [15/50], Step [200/500] Loss: 2.8126\n",
      "Epoch [15/50], Step [300/500] Loss: 2.7550\n",
      "Epoch [15/50], Step [400/500] Loss: 2.7520\n",
      "Epoch [15/50], Step [500/500] Loss: 2.8100\n",
      "Current learning rate:  0.001\n",
      "Epoch [16/50], Step [100/500] Loss: 2.7934\n",
      "Epoch [16/50], Step [200/500] Loss: 2.7821\n",
      "Epoch [16/50], Step [300/500] Loss: 2.4620\n",
      "Epoch [16/50], Step [400/500] Loss: 2.9375\n",
      "Epoch [16/50], Step [500/500] Loss: 2.8916\n",
      "Current learning rate:  0.001\n",
      "Epoch [17/50], Step [100/500] Loss: 2.9542\n",
      "Epoch [17/50], Step [200/500] Loss: 2.8262\n",
      "Epoch [17/50], Step [300/500] Loss: 2.9281\n",
      "Epoch [17/50], Step [400/500] Loss: 2.7324\n",
      "Epoch [17/50], Step [500/500] Loss: 2.6746\n",
      "Current learning rate:  0.001\n",
      "Epoch [18/50], Step [100/500] Loss: 2.7137\n",
      "Epoch [18/50], Step [200/500] Loss: 2.7962\n",
      "Epoch [18/50], Step [300/500] Loss: 2.8150\n",
      "Epoch [18/50], Step [400/500] Loss: 2.6709\n",
      "Epoch [18/50], Step [500/500] Loss: 2.6599\n",
      "Current learning rate:  0.001\n",
      "Epoch [19/50], Step [100/500] Loss: 2.8775\n",
      "Epoch [19/50], Step [200/500] Loss: 2.7810\n",
      "Epoch [19/50], Step [300/500] Loss: 2.5385\n",
      "Epoch [19/50], Step [400/500] Loss: 2.9183\n",
      "Epoch [19/50], Step [500/500] Loss: 2.9859\n",
      "Current learning rate:  0.001\n",
      "Epoch [20/50], Step [100/500] Loss: 2.7081\n",
      "Epoch [20/50], Step [200/500] Loss: 2.8382\n",
      "Epoch [20/50], Step [300/500] Loss: 2.6617\n",
      "Epoch [20/50], Step [400/500] Loss: 2.8311\n",
      "Epoch [20/50], Step [500/500] Loss: 2.5575\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [21/50], Step [100/500] Loss: 2.7306\n",
      "Epoch [21/50], Step [200/500] Loss: 2.7640\n",
      "Epoch [21/50], Step [300/500] Loss: 2.7366\n",
      "Epoch [21/50], Step [400/500] Loss: 2.7939\n",
      "Epoch [21/50], Step [500/500] Loss: 2.6583\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [22/50], Step [100/500] Loss: 2.6697\n",
      "Epoch [22/50], Step [200/500] Loss: 2.7817\n",
      "Epoch [22/50], Step [300/500] Loss: 2.6170\n",
      "Epoch [22/50], Step [400/500] Loss: 2.5512\n",
      "Epoch [22/50], Step [500/500] Loss: 2.7070\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [23/50], Step [100/500] Loss: 2.8027\n",
      "Epoch [23/50], Step [200/500] Loss: 2.8225\n",
      "Epoch [23/50], Step [300/500] Loss: 2.7287\n",
      "Epoch [23/50], Step [400/500] Loss: 2.6804\n",
      "Epoch [23/50], Step [500/500] Loss: 2.8473\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [24/50], Step [100/500] Loss: 2.7906\n",
      "Epoch [24/50], Step [200/500] Loss: 2.6886\n",
      "Epoch [24/50], Step [300/500] Loss: 2.5937\n",
      "Epoch [24/50], Step [400/500] Loss: 2.8348\n",
      "Epoch [24/50], Step [500/500] Loss: 2.5125\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [25/50], Step [100/500] Loss: 2.5747\n",
      "Epoch [25/50], Step [200/500] Loss: 2.6725\n",
      "Epoch [25/50], Step [300/500] Loss: 2.6514\n",
      "Epoch [25/50], Step [400/500] Loss: 2.7507\n",
      "Epoch [25/50], Step [500/500] Loss: 2.8444\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [26/50], Step [100/500] Loss: 2.5873\n",
      "Epoch [26/50], Step [200/500] Loss: 2.6875\n",
      "Epoch [26/50], Step [300/500] Loss: 2.6932\n",
      "Epoch [26/50], Step [400/500] Loss: 2.8622\n",
      "Epoch [26/50], Step [500/500] Loss: 2.6103\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [27/50], Step [100/500] Loss: 2.7371\n",
      "Epoch [27/50], Step [200/500] Loss: 2.8550\n",
      "Epoch [27/50], Step [300/500] Loss: 2.7977\n",
      "Epoch [27/50], Step [400/500] Loss: 2.8495\n",
      "Epoch [27/50], Step [500/500] Loss: 2.6879\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [28/50], Step [100/500] Loss: 2.7227\n",
      "Epoch [28/50], Step [200/500] Loss: 2.7122\n",
      "Epoch [28/50], Step [300/500] Loss: 2.8689\n",
      "Epoch [28/50], Step [400/500] Loss: 2.7455\n",
      "Epoch [28/50], Step [500/500] Loss: 2.7981\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [29/50], Step [100/500] Loss: 2.7606\n",
      "Epoch [29/50], Step [200/500] Loss: 2.9293\n",
      "Epoch [29/50], Step [300/500] Loss: 2.8023\n",
      "Epoch [29/50], Step [400/500] Loss: 2.8051\n",
      "Epoch [29/50], Step [500/500] Loss: 2.6771\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [30/50], Step [100/500] Loss: 2.8528\n",
      "Epoch [30/50], Step [200/500] Loss: 2.7596\n",
      "Epoch [30/50], Step [300/500] Loss: 2.7447\n",
      "Epoch [30/50], Step [400/500] Loss: 2.7505\n",
      "Epoch [30/50], Step [500/500] Loss: 2.8900\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [31/50], Step [100/500] Loss: 2.8175\n",
      "Epoch [31/50], Step [200/500] Loss: 2.6475\n",
      "Epoch [31/50], Step [300/500] Loss: 2.6692\n",
      "Epoch [31/50], Step [400/500] Loss: 2.6825\n",
      "Epoch [31/50], Step [500/500] Loss: 2.6687\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [32/50], Step [100/500] Loss: 2.6510\n",
      "Epoch [32/50], Step [200/500] Loss: 2.6146\n",
      "Epoch [32/50], Step [300/500] Loss: 2.8725\n",
      "Epoch [32/50], Step [400/500] Loss: 2.7059\n",
      "Epoch [32/50], Step [500/500] Loss: 2.5976\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [33/50], Step [100/500] Loss: 2.7103\n",
      "Epoch [33/50], Step [200/500] Loss: 2.6816\n",
      "Epoch [33/50], Step [300/500] Loss: 2.6247\n",
      "Epoch [33/50], Step [400/500] Loss: 2.5311\n",
      "Epoch [33/50], Step [500/500] Loss: 2.7560\n",
      "Current learning rate:  0.0003333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Step [100/500] Loss: 2.7334\n",
      "Epoch [34/50], Step [200/500] Loss: 2.6441\n",
      "Epoch [34/50], Step [300/500] Loss: 2.7575\n",
      "Epoch [34/50], Step [400/500] Loss: 2.6941\n",
      "Epoch [34/50], Step [500/500] Loss: 2.7010\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [35/50], Step [100/500] Loss: 2.8377\n",
      "Epoch [35/50], Step [200/500] Loss: 2.8226\n",
      "Epoch [35/50], Step [300/500] Loss: 2.8312\n",
      "Epoch [35/50], Step [400/500] Loss: 2.7407\n",
      "Epoch [35/50], Step [500/500] Loss: 2.7282\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [36/50], Step [100/500] Loss: 2.8697\n",
      "Epoch [36/50], Step [200/500] Loss: 2.7334\n",
      "Epoch [36/50], Step [300/500] Loss: 2.7546\n",
      "Epoch [36/50], Step [400/500] Loss: 2.6936\n",
      "Epoch [36/50], Step [500/500] Loss: 2.7428\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [37/50], Step [100/500] Loss: 2.6760\n",
      "Epoch [37/50], Step [200/500] Loss: 2.8001\n",
      "Epoch [37/50], Step [300/500] Loss: 2.8856\n",
      "Epoch [37/50], Step [400/500] Loss: 2.6673\n",
      "Epoch [37/50], Step [500/500] Loss: 2.7252\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [38/50], Step [100/500] Loss: 2.7572\n",
      "Epoch [38/50], Step [200/500] Loss: 2.8073\n",
      "Epoch [38/50], Step [300/500] Loss: 2.8102\n",
      "Epoch [38/50], Step [400/500] Loss: 2.7958\n",
      "Epoch [38/50], Step [500/500] Loss: 2.7718\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [39/50], Step [100/500] Loss: 2.7745\n",
      "Epoch [39/50], Step [200/500] Loss: 2.8370\n",
      "Epoch [39/50], Step [300/500] Loss: 2.8947\n",
      "Epoch [39/50], Step [400/500] Loss: 2.7365\n",
      "Epoch [39/50], Step [500/500] Loss: 2.6742\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [40/50], Step [100/500] Loss: 2.7987\n",
      "Epoch [40/50], Step [200/500] Loss: 2.6930\n",
      "Epoch [40/50], Step [300/500] Loss: 2.7516\n",
      "Epoch [40/50], Step [400/500] Loss: 2.7978\n",
      "Epoch [40/50], Step [500/500] Loss: 2.7486\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [41/50], Step [100/500] Loss: 2.7816\n",
      "Epoch [41/50], Step [200/500] Loss: 2.7439\n",
      "Epoch [41/50], Step [300/500] Loss: 2.7685\n",
      "Epoch [41/50], Step [400/500] Loss: 2.7498\n",
      "Epoch [41/50], Step [500/500] Loss: 2.7592\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [42/50], Step [100/500] Loss: 2.7828\n",
      "Epoch [42/50], Step [200/500] Loss: 2.6625\n",
      "Epoch [42/50], Step [300/500] Loss: 2.6154\n",
      "Epoch [42/50], Step [400/500] Loss: 2.8179\n",
      "Epoch [42/50], Step [500/500] Loss: 2.8469\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [43/50], Step [100/500] Loss: 2.8008\n",
      "Epoch [43/50], Step [200/500] Loss: 2.8106\n",
      "Epoch [43/50], Step [300/500] Loss: 2.7223\n",
      "Epoch [43/50], Step [400/500] Loss: 2.6984\n",
      "Epoch [43/50], Step [500/500] Loss: 2.6016\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [44/50], Step [100/500] Loss: 2.8172\n",
      "Epoch [44/50], Step [200/500] Loss: 2.7701\n",
      "Epoch [44/50], Step [300/500] Loss: 2.6362\n",
      "Epoch [44/50], Step [400/500] Loss: 2.8330\n",
      "Epoch [44/50], Step [500/500] Loss: 2.6734\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [45/50], Step [100/500] Loss: 2.6826\n",
      "Epoch [45/50], Step [200/500] Loss: 2.7515\n",
      "Epoch [45/50], Step [300/500] Loss: 2.7068\n",
      "Epoch [45/50], Step [400/500] Loss: 2.8275\n",
      "Epoch [45/50], Step [500/500] Loss: 2.8796\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [46/50], Step [100/500] Loss: 2.8098\n",
      "Epoch [46/50], Step [200/500] Loss: 2.6082\n",
      "Epoch [46/50], Step [300/500] Loss: 2.8375\n",
      "Epoch [46/50], Step [400/500] Loss: 2.7589\n",
      "Epoch [46/50], Step [500/500] Loss: 2.6909\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [47/50], Step [100/500] Loss: 2.7481\n",
      "Epoch [47/50], Step [200/500] Loss: 2.6263\n",
      "Epoch [47/50], Step [300/500] Loss: 2.6692\n",
      "Epoch [47/50], Step [400/500] Loss: 2.7020\n",
      "Epoch [47/50], Step [500/500] Loss: 2.7749\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [48/50], Step [100/500] Loss: 2.6899\n",
      "Epoch [48/50], Step [200/500] Loss: 2.5361\n",
      "Epoch [48/50], Step [300/500] Loss: 2.7600\n",
      "Epoch [48/50], Step [400/500] Loss: 2.8563\n",
      "Epoch [48/50], Step [500/500] Loss: 2.7913\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [49/50], Step [100/500] Loss: 2.7983\n",
      "Epoch [49/50], Step [200/500] Loss: 2.7187\n",
      "Epoch [49/50], Step [300/500] Loss: 2.7606\n",
      "Epoch [49/50], Step [400/500] Loss: 2.6629\n",
      "Epoch [49/50], Step [500/500] Loss: 2.7840\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [50/50], Step [100/500] Loss: 2.7760\n",
      "Epoch [50/50], Step [200/500] Loss: 2.7591\n",
      "Epoch [50/50], Step [300/500] Loss: 2.7376\n",
      "Epoch [50/50], Step [400/500] Loss: 2.6848\n",
      "Epoch [50/50], Step [500/500] Loss: 2.6792\n",
      "Current learning rate:  0.0001111111111111111\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "model4 = train_model(train_loader, model4, num_epochs, error, optimizer, curr_lr, total_step, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layer1\n",
      "\n",
      "layer2\n",
      "\n",
      "layer3\n"
     ]
    }
   ],
   "source": [
    "net_args = {\n",
    "    \"block\" : ResidualBlock,\n",
    "    \"layers\": [3, 4, 6, 3]\n",
    "}\n",
    "\n",
    "model6 = ResNetX(**net_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Epoch [1/50], Step [100/500] Loss: 2.4178\n",
      "Epoch [1/50], Step [200/500] Loss: 2.4513\n",
      "Epoch [1/50], Step [300/500] Loss: 2.3688\n",
      "Epoch [1/50], Step [400/500] Loss: 2.3529\n",
      "Epoch [1/50], Step [500/500] Loss: 2.3530\n",
      "Current learning rate:  0.001\n",
      "Epoch [2/50], Step [100/500] Loss: 2.3721\n",
      "Epoch [2/50], Step [200/500] Loss: 2.3819\n",
      "Epoch [2/50], Step [300/500] Loss: 2.3370\n",
      "Epoch [2/50], Step [400/500] Loss: 2.3484\n",
      "Epoch [2/50], Step [500/500] Loss: 2.3434\n",
      "Current learning rate:  0.001\n",
      "Epoch [3/50], Step [100/500] Loss: 2.3743\n",
      "Epoch [3/50], Step [200/500] Loss: 2.4079\n",
      "Epoch [3/50], Step [300/500] Loss: 2.3831\n",
      "Epoch [3/50], Step [400/500] Loss: 2.3770\n",
      "Epoch [3/50], Step [500/500] Loss: 2.4426\n",
      "Current learning rate:  0.001\n",
      "Epoch [4/50], Step [100/500] Loss: 2.3814\n",
      "Epoch [4/50], Step [200/500] Loss: 2.3416\n",
      "Epoch [4/50], Step [300/500] Loss: 2.3675\n",
      "Epoch [4/50], Step [400/500] Loss: 2.3036\n",
      "Epoch [4/50], Step [500/500] Loss: 2.3887\n",
      "Current learning rate:  0.001\n",
      "Epoch [5/50], Step [100/500] Loss: 2.3780\n",
      "Epoch [5/50], Step [200/500] Loss: 2.3868\n",
      "Epoch [5/50], Step [300/500] Loss: 2.3884\n",
      "Epoch [5/50], Step [400/500] Loss: 2.3277\n",
      "Epoch [5/50], Step [500/500] Loss: 2.3908\n",
      "Current learning rate:  0.001\n",
      "Epoch [6/50], Step [100/500] Loss: 2.4179\n",
      "Epoch [6/50], Step [200/500] Loss: 2.2728\n",
      "Epoch [6/50], Step [300/500] Loss: 2.3773\n",
      "Epoch [6/50], Step [400/500] Loss: 2.3491\n",
      "Epoch [6/50], Step [500/500] Loss: 2.4200\n",
      "Current learning rate:  0.001\n",
      "Epoch [7/50], Step [100/500] Loss: 2.4163\n",
      "Epoch [7/50], Step [200/500] Loss: 2.4080\n",
      "Epoch [7/50], Step [300/500] Loss: 2.3386\n",
      "Epoch [7/50], Step [400/500] Loss: 2.3489\n",
      "Epoch [7/50], Step [500/500] Loss: 2.3438\n",
      "Current learning rate:  0.001\n",
      "Epoch [8/50], Step [100/500] Loss: 2.3304\n",
      "Epoch [8/50], Step [200/500] Loss: 2.3680\n",
      "Epoch [8/50], Step [300/500] Loss: 2.3661\n",
      "Epoch [8/50], Step [400/500] Loss: 2.4035\n",
      "Epoch [8/50], Step [500/500] Loss: 2.3761\n",
      "Current learning rate:  0.001\n",
      "Epoch [9/50], Step [100/500] Loss: 2.3896\n",
      "Epoch [9/50], Step [200/500] Loss: 2.3737\n",
      "Epoch [9/50], Step [300/500] Loss: 2.3752\n",
      "Epoch [9/50], Step [400/500] Loss: 2.3230\n",
      "Epoch [9/50], Step [500/500] Loss: 2.4221\n",
      "Current learning rate:  0.001\n",
      "Epoch [10/50], Step [100/500] Loss: 2.3787\n",
      "Epoch [10/50], Step [200/500] Loss: 2.3605\n",
      "Epoch [10/50], Step [300/500] Loss: 2.4306\n",
      "Epoch [10/50], Step [400/500] Loss: 2.3813\n",
      "Epoch [10/50], Step [500/500] Loss: 2.4420\n",
      "Current learning rate:  0.001\n",
      "Epoch [11/50], Step [100/500] Loss: 2.3049\n",
      "Epoch [11/50], Step [200/500] Loss: 2.3569\n",
      "Epoch [11/50], Step [300/500] Loss: 2.4100\n",
      "Epoch [11/50], Step [400/500] Loss: 2.3773\n",
      "Epoch [11/50], Step [500/500] Loss: 2.3206\n",
      "Current learning rate:  0.001\n",
      "Epoch [12/50], Step [100/500] Loss: 2.3455\n",
      "Epoch [12/50], Step [200/500] Loss: 2.4025\n",
      "Epoch [12/50], Step [300/500] Loss: 2.2891\n",
      "Epoch [12/50], Step [400/500] Loss: 2.3561\n",
      "Epoch [12/50], Step [500/500] Loss: 2.3632\n",
      "Current learning rate:  0.001\n",
      "Epoch [13/50], Step [100/500] Loss: 2.3701\n",
      "Epoch [13/50], Step [200/500] Loss: 2.4083\n",
      "Epoch [13/50], Step [300/500] Loss: 2.3533\n",
      "Epoch [13/50], Step [400/500] Loss: 2.2833\n",
      "Epoch [13/50], Step [500/500] Loss: 2.3567\n",
      "Current learning rate:  0.001\n",
      "Epoch [14/50], Step [100/500] Loss: 2.3244\n",
      "Epoch [14/50], Step [200/500] Loss: 2.4132\n",
      "Epoch [14/50], Step [300/500] Loss: 2.3856\n",
      "Epoch [14/50], Step [400/500] Loss: 2.3762\n",
      "Epoch [14/50], Step [500/500] Loss: 2.4085\n",
      "Current learning rate:  0.001\n",
      "Epoch [15/50], Step [100/500] Loss: 2.4284\n",
      "Epoch [15/50], Step [200/500] Loss: 2.4178\n",
      "Epoch [15/50], Step [300/500] Loss: 2.3936\n",
      "Epoch [15/50], Step [400/500] Loss: 2.4401\n",
      "Epoch [15/50], Step [500/500] Loss: 2.2974\n",
      "Current learning rate:  0.001\n",
      "Epoch [16/50], Step [100/500] Loss: 2.3396\n",
      "Epoch [16/50], Step [200/500] Loss: 2.3369\n",
      "Epoch [16/50], Step [300/500] Loss: 2.3258\n",
      "Epoch [16/50], Step [400/500] Loss: 2.4239\n",
      "Epoch [16/50], Step [500/500] Loss: 2.3429\n",
      "Current learning rate:  0.001\n",
      "Epoch [17/50], Step [100/500] Loss: 2.2871\n",
      "Epoch [17/50], Step [200/500] Loss: 2.3870\n",
      "Epoch [17/50], Step [300/500] Loss: 2.3373\n",
      "Epoch [17/50], Step [400/500] Loss: 2.3629\n",
      "Epoch [17/50], Step [500/500] Loss: 2.3677\n",
      "Current learning rate:  0.001\n",
      "Epoch [18/50], Step [100/500] Loss: 2.4029\n",
      "Epoch [18/50], Step [200/500] Loss: 2.3734\n",
      "Epoch [18/50], Step [300/500] Loss: 2.3612\n",
      "Epoch [18/50], Step [400/500] Loss: 2.3274\n",
      "Epoch [18/50], Step [500/500] Loss: 2.4224\n",
      "Current learning rate:  0.001\n",
      "Epoch [19/50], Step [100/500] Loss: 2.3680\n",
      "Epoch [19/50], Step [200/500] Loss: 2.3390\n",
      "Epoch [19/50], Step [300/500] Loss: 2.3639\n",
      "Epoch [19/50], Step [400/500] Loss: 2.3757\n",
      "Epoch [19/50], Step [500/500] Loss: 2.3984\n",
      "Current learning rate:  0.001\n",
      "Epoch [20/50], Step [100/500] Loss: 2.4060\n",
      "Epoch [20/50], Step [200/500] Loss: 2.4411\n",
      "Epoch [20/50], Step [300/500] Loss: 2.3751\n",
      "Epoch [20/50], Step [400/500] Loss: 2.3599\n",
      "Epoch [20/50], Step [500/500] Loss: 2.3830\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [21/50], Step [100/500] Loss: 2.3611\n",
      "Epoch [21/50], Step [200/500] Loss: 2.3631\n",
      "Epoch [21/50], Step [300/500] Loss: 2.3208\n",
      "Epoch [21/50], Step [400/500] Loss: 2.3808\n",
      "Epoch [21/50], Step [500/500] Loss: 2.4627\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [22/50], Step [100/500] Loss: 2.3526\n",
      "Epoch [22/50], Step [200/500] Loss: 2.3815\n",
      "Epoch [22/50], Step [300/500] Loss: 2.3793\n",
      "Epoch [22/50], Step [400/500] Loss: 2.3399\n",
      "Epoch [22/50], Step [500/500] Loss: 2.3653\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [23/50], Step [100/500] Loss: 2.3238\n",
      "Epoch [23/50], Step [200/500] Loss: 2.4049\n",
      "Epoch [23/50], Step [300/500] Loss: 2.3838\n",
      "Epoch [23/50], Step [400/500] Loss: 2.4169\n",
      "Epoch [23/50], Step [500/500] Loss: 2.3033\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [24/50], Step [100/500] Loss: 2.3568\n",
      "Epoch [24/50], Step [200/500] Loss: 2.3920\n",
      "Epoch [24/50], Step [300/500] Loss: 2.2770\n",
      "Epoch [24/50], Step [400/500] Loss: 2.3613\n",
      "Epoch [24/50], Step [500/500] Loss: 2.3639\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [25/50], Step [100/500] Loss: 2.3410\n",
      "Epoch [25/50], Step [200/500] Loss: 2.3378\n",
      "Epoch [25/50], Step [300/500] Loss: 2.3403\n",
      "Epoch [25/50], Step [400/500] Loss: 2.3873\n",
      "Epoch [25/50], Step [500/500] Loss: 2.3722\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [26/50], Step [100/500] Loss: 2.4293\n",
      "Epoch [26/50], Step [200/500] Loss: 2.3556\n",
      "Epoch [26/50], Step [300/500] Loss: 2.3403\n",
      "Epoch [26/50], Step [400/500] Loss: 2.3334\n",
      "Epoch [26/50], Step [500/500] Loss: 2.3681\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [27/50], Step [100/500] Loss: 2.3893\n",
      "Epoch [27/50], Step [200/500] Loss: 2.3984\n",
      "Epoch [27/50], Step [300/500] Loss: 2.3626\n",
      "Epoch [27/50], Step [400/500] Loss: 2.3524\n",
      "Epoch [27/50], Step [500/500] Loss: 2.3394\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [28/50], Step [100/500] Loss: 2.4106\n",
      "Epoch [28/50], Step [200/500] Loss: 2.4173\n",
      "Epoch [28/50], Step [300/500] Loss: 2.3935\n",
      "Epoch [28/50], Step [400/500] Loss: 2.3545\n",
      "Epoch [28/50], Step [500/500] Loss: 2.3482\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [29/50], Step [100/500] Loss: 2.3181\n",
      "Epoch [29/50], Step [200/500] Loss: 2.4096\n",
      "Epoch [29/50], Step [300/500] Loss: 2.3550\n",
      "Epoch [29/50], Step [400/500] Loss: 2.3800\n",
      "Epoch [29/50], Step [500/500] Loss: 2.3187\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [30/50], Step [100/500] Loss: 2.3790\n",
      "Epoch [30/50], Step [200/500] Loss: 2.4063\n",
      "Epoch [30/50], Step [300/500] Loss: 2.3866\n",
      "Epoch [30/50], Step [400/500] Loss: 2.3501\n",
      "Epoch [30/50], Step [500/500] Loss: 2.3856\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [31/50], Step [100/500] Loss: 2.4012\n",
      "Epoch [31/50], Step [200/500] Loss: 2.3633\n",
      "Epoch [31/50], Step [300/500] Loss: 2.3664\n",
      "Epoch [31/50], Step [400/500] Loss: 2.3919\n",
      "Epoch [31/50], Step [500/500] Loss: 2.3344\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [32/50], Step [100/500] Loss: 2.3891\n",
      "Epoch [32/50], Step [200/500] Loss: 2.3943\n",
      "Epoch [32/50], Step [300/500] Loss: 2.4178\n",
      "Epoch [32/50], Step [400/500] Loss: 2.3807\n",
      "Epoch [32/50], Step [500/500] Loss: 2.3344\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [33/50], Step [100/500] Loss: 2.3711\n",
      "Epoch [33/50], Step [200/500] Loss: 2.3519\n",
      "Epoch [33/50], Step [300/500] Loss: 2.3654\n",
      "Epoch [33/50], Step [400/500] Loss: 2.3342\n",
      "Epoch [33/50], Step [500/500] Loss: 2.3515\n",
      "Current learning rate:  0.0003333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Step [100/500] Loss: 2.3758\n",
      "Epoch [34/50], Step [200/500] Loss: 2.3518\n",
      "Epoch [34/50], Step [300/500] Loss: 2.4662\n",
      "Epoch [34/50], Step [400/500] Loss: 2.4451\n",
      "Epoch [34/50], Step [500/500] Loss: 2.4124\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [35/50], Step [100/500] Loss: 2.4145\n",
      "Epoch [35/50], Step [200/500] Loss: 2.4176\n",
      "Epoch [35/50], Step [300/500] Loss: 2.4160\n",
      "Epoch [35/50], Step [400/500] Loss: 2.4071\n",
      "Epoch [35/50], Step [500/500] Loss: 2.3949\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [36/50], Step [100/500] Loss: 2.4179\n",
      "Epoch [36/50], Step [200/500] Loss: 2.3982\n",
      "Epoch [36/50], Step [300/500] Loss: 2.3337\n",
      "Epoch [36/50], Step [400/500] Loss: 2.3699\n",
      "Epoch [36/50], Step [500/500] Loss: 2.3813\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [37/50], Step [100/500] Loss: 2.3376\n",
      "Epoch [37/50], Step [200/500] Loss: 2.3535\n",
      "Epoch [37/50], Step [300/500] Loss: 2.3285\n",
      "Epoch [37/50], Step [400/500] Loss: 2.3396\n",
      "Epoch [37/50], Step [500/500] Loss: 2.3316\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [38/50], Step [100/500] Loss: 2.3437\n",
      "Epoch [38/50], Step [200/500] Loss: 2.3662\n",
      "Epoch [38/50], Step [300/500] Loss: 2.3621\n",
      "Epoch [38/50], Step [400/500] Loss: 2.3532\n",
      "Epoch [38/50], Step [500/500] Loss: 2.3384\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [39/50], Step [100/500] Loss: 2.4655\n",
      "Epoch [39/50], Step [200/500] Loss: 2.3519\n",
      "Epoch [39/50], Step [300/500] Loss: 2.3697\n",
      "Epoch [39/50], Step [400/500] Loss: 2.3838\n",
      "Epoch [39/50], Step [500/500] Loss: 2.3433\n",
      "Current learning rate:  0.0003333333333333333\n",
      "Epoch [40/50], Step [100/500] Loss: 2.3281\n",
      "Epoch [40/50], Step [200/500] Loss: 2.3651\n",
      "Epoch [40/50], Step [300/500] Loss: 2.3319\n",
      "Epoch [40/50], Step [400/500] Loss: 2.3099\n",
      "Epoch [40/50], Step [500/500] Loss: 2.3145\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [41/50], Step [100/500] Loss: 2.3085\n",
      "Epoch [41/50], Step [200/500] Loss: 2.3784\n",
      "Epoch [41/50], Step [300/500] Loss: 2.3386\n",
      "Epoch [41/50], Step [400/500] Loss: 2.3919\n",
      "Epoch [41/50], Step [500/500] Loss: 2.3790\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [42/50], Step [100/500] Loss: 2.4349\n",
      "Epoch [42/50], Step [200/500] Loss: 2.3601\n",
      "Epoch [42/50], Step [300/500] Loss: 2.2994\n",
      "Epoch [42/50], Step [400/500] Loss: 2.3581\n",
      "Epoch [42/50], Step [500/500] Loss: 2.3767\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [43/50], Step [100/500] Loss: 2.4234\n",
      "Epoch [43/50], Step [200/500] Loss: 2.2962\n",
      "Epoch [43/50], Step [300/500] Loss: 2.3333\n",
      "Epoch [43/50], Step [400/500] Loss: 2.4073\n",
      "Epoch [43/50], Step [500/500] Loss: 2.3736\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [44/50], Step [100/500] Loss: 2.3899\n",
      "Epoch [44/50], Step [200/500] Loss: 2.3627\n",
      "Epoch [44/50], Step [300/500] Loss: 2.3714\n",
      "Epoch [44/50], Step [400/500] Loss: 2.3989\n",
      "Epoch [44/50], Step [500/500] Loss: 2.3216\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [45/50], Step [100/500] Loss: 2.4011\n",
      "Epoch [45/50], Step [200/500] Loss: 2.3657\n",
      "Epoch [45/50], Step [300/500] Loss: 2.3788\n",
      "Epoch [45/50], Step [400/500] Loss: 2.3883\n",
      "Epoch [45/50], Step [500/500] Loss: 2.4361\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [46/50], Step [100/500] Loss: 2.3747\n",
      "Epoch [46/50], Step [200/500] Loss: 2.3054\n",
      "Epoch [46/50], Step [300/500] Loss: 2.3283\n",
      "Epoch [46/50], Step [400/500] Loss: 2.2951\n",
      "Epoch [46/50], Step [500/500] Loss: 2.4279\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [47/50], Step [100/500] Loss: 2.3920\n",
      "Epoch [47/50], Step [200/500] Loss: 2.4034\n",
      "Epoch [47/50], Step [300/500] Loss: 2.3834\n",
      "Epoch [47/50], Step [400/500] Loss: 2.4137\n",
      "Epoch [47/50], Step [500/500] Loss: 2.3983\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [48/50], Step [100/500] Loss: 2.4043\n",
      "Epoch [48/50], Step [200/500] Loss: 2.3673\n",
      "Epoch [48/50], Step [300/500] Loss: 2.3546\n",
      "Epoch [48/50], Step [400/500] Loss: 2.3777\n",
      "Epoch [48/50], Step [500/500] Loss: 2.2956\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [49/50], Step [100/500] Loss: 2.3193\n",
      "Epoch [49/50], Step [200/500] Loss: 2.2960\n",
      "Epoch [49/50], Step [300/500] Loss: 2.3953\n",
      "Epoch [49/50], Step [400/500] Loss: 2.4057\n",
      "Epoch [49/50], Step [500/500] Loss: 2.3425\n",
      "Current learning rate:  0.0001111111111111111\n",
      "Epoch [50/50], Step [100/500] Loss: 2.3646\n",
      "Epoch [50/50], Step [200/500] Loss: 2.3444\n",
      "Epoch [50/50], Step [300/500] Loss: 2.3201\n",
      "Epoch [50/50], Step [400/500] Loss: 2.3833\n",
      "Epoch [50/50], Step [500/500] Loss: 2.3364\n",
      "Current learning rate:  0.0001111111111111111\n"
     ]
    }
   ],
   "source": [
    "model6 = train_model(train_loader, model6, num_epochs, error, optimizer, curr_lr, total_step, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device): \n",
    "    print(\"Testing model\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.resize_(100, 1, 32, 32).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Accuracy of the model on the test images: 9.87 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Accuracy of the model on the test images: 10.47 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model2, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Accuracy of the model on the test images: 10.0 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model3, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Accuracy of the model on the test images: 9.99 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model4, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Accuracy of the model on the test images: 10.05 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model6, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = model5.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "transforms.Resize(256),\n",
    "transforms.CenterCrop(224),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(\n",
    "mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225]\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
